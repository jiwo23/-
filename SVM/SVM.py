#!/usr/bin/env python
# coding: utf-8

# In[4]:


get_ipython().system('pip install jieba pandas numpy scikit-learn matplotlib seaborn')


# In[18]:


# ==================== 1. å¯¼å…¥å¿…è¦çš„åº“ ====================
import pandas as pd
import numpy as np
import jieba
import re
import os
import warnings
warnings.filterwarnings('ignore')

# æœºå™¨å­¦ä¹ ç›¸å…³åº“
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score

# å¯è§†åŒ–åº“
import matplotlib.pyplot as plt
import seaborn as sns

# è®¾ç½®ä¸­æ–‡å­—ä½“
try:
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
except:
    pass

print("æ‰€æœ‰å¿…è¦çš„åº“å·²å¯¼å…¥")
print("=" * 60)

# ==================== 2. XMLæ–‡ä»¶è§£æå‡½æ•° ====================
def parse_xml_file(file_path):
    """
    è§£æXMLæ ¼å¼çš„è¯„è®ºæ–‡ä»¶
    æ ¼å¼: <review id="10">è¯„è®ºå†…å®¹</review>
    è¿”å›: è¯„è®ºåˆ—è¡¨
    """
    print(f"è§£æXMLæ–‡ä»¶: {file_path}")
    reviews = []
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except UnicodeDecodeError:
        # å°è¯•ä½¿ç”¨GBKç¼–ç 
        try:
            with open(file_path, 'r', encoding='gbk') as f:
                content = f.read()
        except:
            print(f"  é”™è¯¯ï¼šæ— æ³•è¯»å–æ–‡ä»¶ {file_path}ï¼Œè¯·æ£€æŸ¥ç¼–ç æ ¼å¼")
            return []
    
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ‰€æœ‰reviewæ ‡ç­¾å†…çš„å†…å®¹
    # åŒ¹é…æ¨¡å¼: <review id="æ•°å­—">å†…å®¹</review>
    pattern = r'<review[^>]*>(.*?)</review>'
    matches = re.findall(pattern, content, re.DOTALL)  # re.DOTALLä½¿.åŒ¹é…åŒ…æ‹¬æ¢è¡Œç¬¦åœ¨å†…çš„æ‰€æœ‰å­—ç¬¦
    
    for i, match in enumerate(matches):
        # æ¸…ç†å†…å®¹ï¼šå»é™¤å‰åç©ºç™½ï¼Œåˆå¹¶æ¢è¡Œç¬¦
        text = match.strip()
        # å°†æ¢è¡Œç¬¦æ›¿æ¢ä¸ºç©ºæ ¼
        text = re.sub(r'\s+', ' ', text)
        reviews.append(text)
    
    print(f"  æ‰¾åˆ° {len(reviews)} æ¡è¯„è®º")
    
    # æ˜¾ç¤ºå‰3æ¡è¯„è®ºä½œä¸ºç¤ºä¾‹
    if len(reviews) >= 3:
        print("  å‰3æ¡è¯„è®ºç¤ºä¾‹:")
        for i in range(min(3, len(reviews))):
            print(f"    è¯„è®º{i+1}: {reviews[i][:50]}...")
    
    return reviews

def parse_xml_label_file(file_path):
    """
    è§£æXMLæ ¼å¼çš„æ ‡ç­¾æ–‡ä»¶
    æ ¼å¼: <review id="0"  label="0">
    è¿”å›: æ ‡ç­¾åˆ—è¡¨ (0æˆ–1)
    """
    print(f"è§£æXMLæ ‡ç­¾æ–‡ä»¶: {file_path}")
    labels = []
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except UnicodeDecodeError:
        # å°è¯•ä½¿ç”¨GBKç¼–ç 
        try:
            with open(file_path, 'r', encoding='gbk') as f:
                content = f.read()
        except:
            print(f"  é”™è¯¯ï¼šæ— æ³•è¯»å–æ–‡ä»¶ {file_path}ï¼Œè¯·æ£€æŸ¥ç¼–ç æ ¼å¼")
            return []
    
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ‰€æœ‰reviewæ ‡ç­¾çš„labelå±æ€§
    # åŒ¹é…æ¨¡å¼: <review id="æ•°å­—" label="æ•°å­—">
    pattern = r'<review[^>]*label\s*=\s*"(\d+)"[^>]*>'
    matches = re.findall(pattern, content)
    
    for i, match in enumerate(matches):
        try:
            label = int(match)
            if label not in [0, 1]:
                label = label % 2  # å¦‚æœä¸æ˜¯0æˆ–1ï¼Œå–æ¨¡2
                print(f"  è­¦å‘Šï¼šç¬¬{i+1}ä¸ªæ ‡ç­¾å€¼ {match} ä¸æ˜¯0æˆ–1ï¼Œå·²è½¬æ¢ä¸º {label}")
            labels.append(label)
        except ValueError:
            # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œè®¾ä¸ºé»˜è®¤å€¼0
            labels.append(0)
            print(f"  é”™è¯¯ï¼šç¬¬{i+1}ä¸ªæ ‡ç­¾æ— æ³•è½¬æ¢ï¼Œè®¾ä¸ºé»˜è®¤å€¼0")
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°labelå±æ€§ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
    if len(labels) == 0:
        print("  æœªæ‰¾åˆ°labelå±æ€§ï¼Œå°è¯•å…¶ä»–æå–æ–¹æ³•...")
        # æå–æ‰€æœ‰reviewæ ‡ç­¾
        pattern = r'<review[^>]*>'
        matches = re.findall(pattern, content)
        
        for i, match in enumerate(matches):
            # æå–æ‰€æœ‰æ•°å­—
            numbers = re.findall(r'\d+', match)
            if numbers:
                try:
                    label = int(numbers[0]) % 2
                    labels.append(label)
                except ValueError:
                    labels.append(0)
            else:
                labels.append(0)
    
    print(f"  æ‰¾åˆ° {len(labels)} ä¸ªæ ‡ç­¾")
    
    # æ˜¾ç¤ºæ ‡ç­¾åˆ†å¸ƒ
    if labels:
        pos_count = sum(labels)
        neg_count = len(labels) - pos_count
        print(f"  æ ‡ç­¾åˆ†å¸ƒ: æ­£é¢={pos_count} ({pos_count/len(labels):.2%}), è´Ÿé¢={neg_count} ({neg_count/len(labels):.2%})")
    
    return labels

def parse_plain_text_file(file_path):
    """
    è§£æçº¯æ–‡æœ¬æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€æ¡è¯„è®ºï¼‰
    è¿”å›: è¯„è®ºåˆ—è¡¨
    """
    print(f"è§£æçº¯æ–‡æœ¬æ–‡ä»¶: {file_path}")
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
    except UnicodeDecodeError:
        # å°è¯•ä½¿ç”¨GBKç¼–ç 
        try:
            with open(file_path, 'r', encoding='gbk') as f:
                lines = f.readlines()
        except:
            print(f"  é”™è¯¯ï¼šæ— æ³•è¯»å–æ–‡ä»¶ {file_path}ï¼Œè¯·æ£€æŸ¥ç¼–ç æ ¼å¼")
            return []
    
    # æ¸…ç†æ¯è¡Œï¼šå»é™¤ç©ºç™½å­—ç¬¦
    reviews = [line.strip() for line in lines if line.strip()]
    
    print(f"  æ‰¾åˆ° {len(reviews)} æ¡è¯„è®º")
    
    # æ˜¾ç¤ºå‰3æ¡è¯„è®ºä½œä¸ºç¤ºä¾‹
    if len(reviews) >= 3:
        print("  å‰3æ¡è¯„è®ºç¤ºä¾‹:")
        for i in range(min(3, len(reviews))):
            print(f"    è¯„è®º{i+1}: {reviews[i][:50]}...")
    
    return reviews

# ==================== 3. æ•°æ®åŠ è½½å‡½æ•° ====================
def load_data():
    """
    åŠ è½½æ‰€æœ‰æ•°æ®æ–‡ä»¶
    è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶æ ¼å¼å¹¶é€‰æ‹©æ­£ç¡®çš„è§£ææ–¹æ³•
    è¿”å›: è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„DataFrame
    """
    print("æ­£åœ¨åŠ è½½æ•°æ®æ–‡ä»¶...")
    
    # å®šä¹‰æ–‡ä»¶åˆ—è¡¨
    files = ['sample.positive.txt', 'sample.negative.txt', 'test.cn.txt', 'test.label.cn.txt']
    missing_files = []
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    for file in files:
        if not os.path.exists(file):
            missing_files.append(file)
            print(f"  âœ— æœªæ‰¾åˆ°: {file}")
        else:
            print(f"  âœ“ æ‰¾åˆ°: {file}")
    
    # å¦‚æœæœ‰æ–‡ä»¶ç¼ºå¤±ï¼Œæç¤ºç”¨æˆ·
    if missing_files:
        print(f"\né”™è¯¯ï¼šä»¥ä¸‹æ–‡ä»¶æœªæ‰¾åˆ°: {missing_files}")
        return None, None
    
    try:
        # æ£€æµ‹æ–‡ä»¶æ ¼å¼å¹¶é€‰æ‹©æ­£ç¡®çš„è§£ææ–¹æ³•
        def detect_file_format(file_path):
            """æ£€æµ‹æ–‡ä»¶æ˜¯XMLæ ¼å¼è¿˜æ˜¯çº¯æ–‡æœ¬æ ¼å¼"""
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    first_line = f.readline().strip()
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«XMLæ ‡ç­¾
                    if first_line.startswith('<review') and '>' in first_line:
                        return 'xml'
                    else:
                        # æ£€æŸ¥æ•´ä¸ªæ–‡ä»¶æ˜¯å¦æœ‰XMLæ ‡ç­¾
                        f.seek(0)
                        content = f.read(1000)  # è¯»å–å‰1000ä¸ªå­—ç¬¦
                        if '<review' in content and '</review>' in content:
                            return 'xml'
                        else:
                            return 'text'
            except UnicodeDecodeError:
                # å°è¯•GBKç¼–ç 
                try:
                    with open(file_path, 'r', encoding='gbk') as f:
                        first_line = f.readline().strip()
                        if first_line.startswith('<review') and '>' in first_line:
                            return 'xml'
                        else:
                            f.seek(0)
                            content = f.read(1000)
                            if '<review' in content and '</review>' in content:
                                return 'xml'
                            else:
                                return 'text'
                except:
                    return 'text'  # é»˜è®¤æŒ‰æ–‡æœ¬å¤„ç†
        
        print("\næ£€æµ‹æ–‡ä»¶æ ¼å¼...")
        
        # 1. æ£€æµ‹å¹¶è¯»å–æ­£é¢è®­ç»ƒé›†
        positive_format = detect_file_format('sample.positive.txt')
        print(f"  sample.positive.txt æ ¼å¼: {positive_format}")
        
        if positive_format == 'xml':
            positive_reviews = parse_xml_file('sample.positive.txt')
        else:
            positive_reviews = parse_plain_text_file('sample.positive.txt')
        
        # 2. æ£€æµ‹å¹¶è¯»å–è´Ÿé¢è®­ç»ƒé›†
        negative_format = detect_file_format('sample.negative.txt')
        print(f"  sample.negative.txt æ ¼å¼: {negative_format}")
        
        if negative_format == 'xml':
            negative_reviews = parse_xml_file('sample.negative.txt')
        else:
            negative_reviews = parse_plain_text_file('sample.negative.txt')
        
        # 3. æ£€æµ‹å¹¶è¯»å–æµ‹è¯•é›†
        test_format = detect_file_format('test.cn.txt')
        print(f"  test.cn.txt æ ¼å¼: {test_format}")
        
        if test_format == 'xml':
            test_reviews = parse_xml_file('test.cn.txt')
        else:
            test_reviews = parse_plain_text_file('test.cn.txt')
        
        # 4. è¯»å–æµ‹è¯•é›†æ ‡ç­¾ (æ€»æ˜¯æŒ‰XMLæ ¼å¼è§£æï¼Œå› ä¸ºæ‚¨æåˆ°æ˜¯XMLæ ¼å¼)
        print(f"  test.label.cn.txt æ ¼å¼: xml (å¼ºåˆ¶)")
        test_labels = parse_xml_label_file('test.label.cn.txt')
        
        # æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§
        if len(test_reviews) != len(test_labels):
            print(f"\nè­¦å‘Šï¼šæµ‹è¯•é›†è¯„è®ºæ•°({len(test_reviews)})ä¸æ ‡ç­¾æ•°({len(test_labels)})ä¸åŒ¹é…ï¼")
            # å–è¾ƒå°å€¼
            min_len = min(len(test_reviews), len(test_labels))
            test_reviews = test_reviews[:min_len]
            test_labels = test_labels[:min_len]
            print(f"  è°ƒæ•´ä¸º {min_len} æ¡æµ‹è¯•æ•°æ®")
        
        # åˆ›å»ºè®­ç»ƒé›†DataFrame
        train_reviews = positive_reviews + negative_reviews
        train_labels = [1] * len(positive_reviews) + [0] * len(negative_reviews)
        
        train_df = pd.DataFrame({
            'review': train_reviews,
            'label': train_labels
        })
        
        # åˆ›å»ºæµ‹è¯•é›†DataFrame
        test_df = pd.DataFrame({
            'review': test_reviews,
            'label': test_labels
        })
        
        print(f"\nâœ“ æ•°æ®åŠ è½½å®Œæˆï¼")
        print(f"  è®­ç»ƒé›†å¤§å°: {len(train_df)} (æ­£é¢: {len(positive_reviews)}, è´Ÿé¢: {len(negative_reviews)})")
        print(f"  æµ‹è¯•é›†å¤§å°: {len(test_df)}")
        
        # æ˜¾ç¤ºæ ‡ç­¾åˆ†å¸ƒ
        print(f"\nğŸ“Š æ ‡ç­¾åˆ†å¸ƒ:")
        train_pos = train_df['label'].sum()
        train_neg = len(train_df) - train_pos
        print(f"  è®­ç»ƒé›† - æ­£é¢: {train_pos} ({train_pos/len(train_df):.2%}), è´Ÿé¢: {train_neg} ({train_neg/len(train_df):.2%})")
        
        test_pos = test_df['label'].sum()
        test_neg = len(test_df) - test_pos
        print(f"  æµ‹è¯•é›† - æ­£é¢: {test_pos} ({test_pos/len(test_df):.2%}), è´Ÿé¢: {test_neg} ({test_neg/len(test_df):.2%})")
        
        return train_df, test_df
        
    except Exception as e:
        print(f"\né”™è¯¯ï¼šè¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return None, None

# åŠ è½½æ•°æ®
print("\n" + "=" * 60)
print("æ•°æ®åŠ è½½")
print("=" * 60)
train_df, test_df = load_data()

# å¦‚æœæ•°æ®åŠ è½½å¤±è´¥ï¼Œæ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
if train_df is None or test_df is None:
    print("æ•°æ®åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨å’Œæ ¼å¼æ˜¯å¦æ­£ç¡®")
    # é€€å‡ºç¨‹åº
    raise SystemExit("ç¨‹åºç»ˆæ­¢ï¼šæ•°æ®åŠ è½½å¤±è´¥")

# æ˜¾ç¤ºæ•°æ®å‰å‡ è¡Œ
print("\nè®­ç»ƒé›†å‰5è¡Œ:")
print(train_df.head())
print("\næµ‹è¯•é›†å‰5è¡Œ:")
print(test_df.head())

# ==================== 4. æ•°æ®åˆ†æå’Œå¯è§†åŒ– ====================
print("\n" + "=" * 60)
print("æ•°æ®é›†åˆ†æ")
print("=" * 60)

# è®­ç»ƒé›†åˆ†æ
print("\nğŸ“Š è®­ç»ƒé›†ç»Ÿè®¡:")
print(f"  æ€»æ ·æœ¬æ•°: {len(train_df)}")
print(f"  æ­£é¢æ ·æœ¬æ•°: {train_df['label'].sum()}")
print(f"  è´Ÿé¢æ ·æœ¬æ•°: {len(train_df) - train_df['label'].sum()}")
print(f"  æ­£é¢æ¯”ä¾‹: {train_df['label'].mean():.2%}")
print(f"  è´Ÿé¢æ¯”ä¾‹: {(1 - train_df['label'].mean()):.2%}")

# æµ‹è¯•é›†åˆ†æ
print("\nğŸ“Š æµ‹è¯•é›†ç»Ÿè®¡:")
print(f"  æ€»æ ·æœ¬æ•°: {len(test_df)}")
print(f"  æ­£é¢æ ·æœ¬æ•°: {test_df['label'].sum()}")
print(f"  è´Ÿé¢æ ·æœ¬æ•°: {len(test_df) - test_df['label'].sum()}")
print(f"  æ­£é¢æ¯”ä¾‹: {test_df['label'].mean():.2%}")
print(f"  è´Ÿé¢æ¯”ä¾‹: {(1 - test_df['label'].mean()):.2%}")

# æ–‡æœ¬é•¿åº¦åˆ†æ
train_df['review_length'] = train_df['review'].apply(len)
test_df['review_length'] = test_df['review'].apply(len)

print("\nğŸ“Š æ–‡æœ¬é•¿åº¦ç»Ÿè®¡:")
print("  è®­ç»ƒé›†:")
print(f"    å¹³å‡é•¿åº¦: {train_df['review_length'].mean():.1f} å­—ç¬¦")
print(f"    æœ€å°é•¿åº¦: {train_df['review_length'].min()} å­—ç¬¦")
print(f"    æœ€å¤§é•¿åº¦: {train_df['review_length'].max()} å­—ç¬¦")
print(f"    é•¿åº¦ä¸­ä½æ•°: {train_df['review_length'].median()} å­—ç¬¦")
print("  æµ‹è¯•é›†:")
print(f"    å¹³å‡é•¿åº¦: {test_df['review_length'].mean():.1f} å­—ç¬¦")
print(f"    æœ€å°é•¿åº¦: {test_df['review_length'].min()} å­—ç¬¦")
print(f"    æœ€å¤§é•¿åº¦: {test_df['review_length'].max()} å­—ç¬¦")
print(f"    é•¿åº¦ä¸­ä½æ•°: {test_df['review_length'].median()} å­—ç¬¦")

# å¯è§†åŒ–
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# 1. è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ
ax1 = axes[0, 0]
train_counts = train_df['label'].value_counts()
ax1.bar(['è´Ÿé¢ (0)', 'æ­£é¢ (1)'], train_counts.values, color=['#FF6B6B', '#4ECDC4'])
ax1.set_title('è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ', fontsize=14)
ax1.set_ylabel('æ ·æœ¬æ•°', fontsize=12)
for i, v in enumerate(train_counts.values):
    ax1.text(i, v + max(train_counts.values)*0.01, str(v), ha='center', fontsize=12)

# 2. æµ‹è¯•é›†æ ‡ç­¾åˆ†å¸ƒ
ax2 = axes[0, 1]
test_counts = test_df['label'].value_counts()
ax2.bar(['è´Ÿé¢ (0)', 'æ­£é¢ (1)'], test_counts.values, color=['#FF6B6B', '#4ECDC4'])
ax2.set_title('æµ‹è¯•é›†æ ‡ç­¾åˆ†å¸ƒ', fontsize=14)
ax2.set_ylabel('æ ·æœ¬æ•°', fontsize=12)
for i, v in enumerate(test_counts.values):
    ax2.text(i, v + max(test_counts.values)*0.01, str(v), ha='center', fontsize=12)

# 3. è®­ç»ƒé›†æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ
ax3 = axes[1, 0]
ax3.hist(train_df['review_length'], bins=30, color='#45B7D1', alpha=0.7, edgecolor='black')
ax3.set_title('è®­ç»ƒé›†æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ', fontsize=14)
ax3.set_xlabel('æ–‡æœ¬é•¿åº¦ (å­—ç¬¦æ•°)', fontsize=12)
ax3.set_ylabel('é¢‘ç‡', fontsize=12)
ax3.axvline(train_df['review_length'].mean(), color='red', linestyle='dashed', linewidth=1, label=f'å¹³å‡: {train_df["review_length"].mean():.1f}')
ax3.legend()

# 4. æµ‹è¯•é›†æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ
ax4 = axes[1, 1]
ax4.hist(test_df['review_length'], bins=30, color='#96CEB4', alpha=0.7, edgecolor='black')
ax4.set_title('æµ‹è¯•é›†æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ', fontsize=14)
ax4.set_xlabel('æ–‡æœ¬é•¿åº¦ (å­—ç¬¦æ•°)', fontsize=12)
ax4.set_ylabel('é¢‘ç‡', fontsize=12)
ax4.axvline(test_df['review_length'].mean(), color='red', linestyle='dashed', linewidth=1, label=f'å¹³å‡: {test_df["review_length"].mean():.1f}')
ax4.legend()

plt.suptitle('æ•°æ®é›†åˆ†æ', fontsize=16, y=1.02)
plt.tight_layout()
plt.show()

# ==================== 5. ä¸­æ–‡æ–‡æœ¬é¢„å¤„ç† ====================
def chinese_preprocess(text):
    """ä¸­æ–‡æ–‡æœ¬é¢„å¤„ç†"""
    if not isinstance(text, str):
        return ""
    
    # å»é™¤HTMLæ ‡ç­¾
    text = re.sub(r'<[^>]+>', '', text)
    
    # å»é™¤URLé“¾æ¥
    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
    
    # å»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—
    text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', '', text)
    
    # å»é™¤å¤šä½™ç©ºç™½å­—ç¬¦
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def chinese_tokenize(text):
    """ä¸­æ–‡åˆ†è¯"""
    if not text:
        return []
    
    # ä½¿ç”¨jiebaè¿›è¡Œåˆ†è¯
    tokens = jieba.lcut(text)
    
    # è¿‡æ»¤æ‰å•å­—ç¬¦ï¼ˆé™¤éæ˜¯ä¸­æ–‡å•å­—ï¼‰
    tokens = [token for token in tokens if len(token) > 1 or '\u4e00' <= token <= '\u9fff']
    
    return tokens

print("\n" + "=" * 60)
print("æ–‡æœ¬é¢„å¤„ç†å’Œç‰¹å¾æå–")
print("=" * 60)

print("æ­£åœ¨è¿›è¡Œæ–‡æœ¬é¢„å¤„ç†...")

# åº”ç”¨æ–‡æœ¬é¢„å¤„ç†
train_df['cleaned_review'] = train_df['review'].apply(chinese_preprocess)
test_df['cleaned_review'] = test_df['review'].apply(chinese_preprocess)

# æ˜¾ç¤ºé¢„å¤„ç†å‰åçš„ç¤ºä¾‹
print("\nğŸ“ é¢„å¤„ç†ç¤ºä¾‹:")
if len(train_df) > 0:
    sample_idx = 0
    print("åŸå§‹è¯„è®º:", train_df['review'].iloc[sample_idx][:50] + "..." if len(train_df['review'].iloc[sample_idx]) > 50 else train_df['review'].iloc[sample_idx])
    print("é¢„å¤„ç†å:", train_df['cleaned_review'].iloc[sample_idx][:50] + "..." if len(train_df['cleaned_review'].iloc[sample_idx]) > 50 else train_df['cleaned_review'].iloc[sample_idx])

# ==================== 6. ç‰¹å¾æå– ====================
# å®šä¹‰åˆ†è¯å‡½æ•°
def tokenizer(text):
    return chinese_tokenize(text)

print("\næ­£åœ¨æå–TF-IDFç‰¹å¾...")

# ä½¿ç”¨TF-IDFå‘é‡åŒ–
vectorizer = TfidfVectorizer(
    tokenizer=tokenizer,
    max_features=5000,  # é™åˆ¶ç‰¹å¾æ•°é‡
    ngram_range=(1, 2),  # ä½¿ç”¨å•å­—å’ŒåŒå­—
    min_df=2,  # æœ€å°æ–‡æ¡£é¢‘ç‡
    max_df=0.9,  # æœ€å¤§æ–‡æ¡£é¢‘ç‡
    sublinear_tf=True  # ä½¿ç”¨å­çº¿æ€§TFç¼©æ”¾
)

# è½¬æ¢è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train = vectorizer.fit_transform(train_df['cleaned_review'])
X_test = vectorizer.transform(test_df['cleaned_review'])

y_train = train_df['label'].values
y_test = test_df['label'].values

print(f"âœ“ ç‰¹å¾æå–å®Œæˆ!")
print(f"  è®­ç»ƒé›†ç‰¹å¾å½¢çŠ¶: {X_train.shape}")
print(f"  æµ‹è¯•é›†ç‰¹å¾å½¢çŠ¶: {X_test.shape}")
print(f"  ç‰¹å¾æ•°é‡: {X_train.shape[1]}")

# ==================== 7. SVMæ¨¡å‹è®­ç»ƒå’Œè¯„ä¼° ====================
print("\n" + "=" * 60)
print("SVMæ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°")
print("=" * 60)

print("æ­£åœ¨è®­ç»ƒSVMæ¨¡å‹...")

# è®­ç»ƒSVMæ¨¡å‹
svm_model = SVC(
    kernel='linear',  # ä½¿ç”¨çº¿æ€§æ ¸ï¼Œé€‚åˆæ–‡æœ¬åˆ†ç±»
    C=1.0,  # æ­£åˆ™åŒ–å‚æ•°
    probability=True,  # å¯ç”¨æ¦‚ç‡é¢„æµ‹
    random_state=42,  # éšæœºç§å­
    verbose=False  # ä¸æ˜¾ç¤ºè®­ç»ƒè¿‡ç¨‹
)

svm_model.fit(X_train, y_train)
print("âœ“ SVMæ¨¡å‹è®­ç»ƒå®Œæˆ!")

# é¢„æµ‹
y_pred = svm_model.predict(X_test)
y_pred_proba = svm_model.predict_proba(X_test)[:, 1]

# è¯„ä¼°æ¨¡å‹
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f"\nğŸ“ˆ æ¨¡å‹æ€§èƒ½æŒ‡æ ‡:")
print(f"  å‡†ç¡®ç‡ (Accuracy): {accuracy:.4f}")
print(f"  F1åˆ†æ•°: {f1:.4f}")

print(f"\nğŸ“Š åˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, y_pred, target_names=['è´Ÿé¢', 'æ­£é¢'], digits=4))

# æ··æ·†çŸ©é˜µ
cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['è´Ÿé¢', 'æ­£é¢'], 
            yticklabels=['è´Ÿé¢', 'æ­£é¢'])
plt.title('SVMæ¨¡å‹ - æ··æ·†çŸ©é˜µ', fontsize=14)
plt.ylabel('çœŸå®æ ‡ç­¾', fontsize=12)
plt.xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=12)
plt.show()

# æ˜¾ç¤ºä¸€äº›é¢„æµ‹ç¤ºä¾‹
print("\nğŸ“‹ é¢„æµ‹ç¤ºä¾‹ï¼ˆå‰10æ¡æµ‹è¯•æ•°æ®ï¼‰:")
print("-" * 80)

for i in range(min(10, len(y_test))):
    if len(test_df['review']) > i:
        review_preview = test_df['review'].iloc[i][:40] + "..." if len(test_df['review'].iloc[i]) > 40 else test_df['review'].iloc[i]
        true_label = "æ­£é¢" if y_test[i] == 1 else "è´Ÿé¢"
        pred_label = "æ­£é¢" if y_pred[i] == 1 else "è´Ÿé¢"
        prob = y_pred_proba[i] if i < len(y_pred_proba) else 0
        
        # æ ‡è®°é¢„æµ‹æ­£ç¡®/é”™è¯¯
        if true_label == pred_label:
            marker = "âœ“"
            color = "\033[92m"  # ç»¿è‰²
        else:
            marker = "âœ—"
            color = "\033[91m"  # çº¢è‰²
        
        print(f"{color}{marker} æ ·æœ¬ {i+1}:")
        print(f"    è¯„è®º: {review_preview}")
        print(f"    çœŸå®: {true_label} | é¢„æµ‹: {pred_label} | æ­£é¢æ¦‚ç‡: {prob:.4f}")
        print("\033[0m" + "-" * 80)

# ==================== 8. ç‰¹å¾é‡è¦æ€§åˆ†æï¼ˆä¿®å¤ç‰ˆï¼‰====================
def analyze_feature_importance(model, vectorizer, top_n=20):
    """åˆ†æç‰¹å¾é‡è¦æ€§"""
    print("\n" + "=" * 60)
    print("ç‰¹å¾é‡è¦æ€§åˆ†æ")
    print("=" * 60)
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æœ‰ç³»æ•°å±æ€§
    if hasattr(model, 'coef_'):
        # è·å–ç‰¹å¾åç§°
        if hasattr(vectorizer, 'get_feature_names_out'):
            feature_names = vectorizer.get_feature_names_out()
        elif hasattr(vectorizer, 'get_feature_names'):
            feature_names = vectorizer.get_feature_names()
        else:
            print("æ— æ³•è·å–ç‰¹å¾åç§°")
            return
        
        # è·å–ç³»æ•°
        coefficients = model.coef_[0] if len(model.coef_.shape) > 1 else model.coef_
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç¨€ç–çŸ©é˜µï¼Œå¦‚æœæ˜¯åˆ™è½¬æ¢ä¸ºå¯†é›†æ•°ç»„
        from scipy.sparse import issparse
        if issparse(coefficients):
            coefficients = coefficients.toarray().flatten()
            print("  æ³¨æ„ï¼šç³»æ•°æ˜¯ç¨€ç–çŸ©é˜µï¼Œå·²è½¬æ¢ä¸ºå¯†é›†æ•°ç»„")
        else:
            coefficients = coefficients.flatten()
        
        # åˆ›å»ºç‰¹å¾é‡è¦æ€§DataFrame
        feature_importance = pd.DataFrame({
            'feature': feature_names,
            'importance': coefficients
        })
        
        # æŒ‰é‡è¦æ€§æ’åº
        feature_importance = feature_importance.sort_values('importance', ascending=False)
        
        # æ˜¾ç¤ºæœ€é‡è¦çš„ç‰¹å¾
        print(f"\nğŸ” æœ€é‡è¦çš„æ­£é¢ç‰¹å¾ï¼ˆå¯¹æ­£é¢åˆ†ç±»è´¡çŒ®æœ€å¤§ï¼Œå‰{top_n}ä¸ªï¼‰:")
        top_positive = feature_importance.head(top_n)
        for i, (idx, row) in enumerate(top_positive.iterrows(), 1):
            print(f"  {i:2d}. {row['feature']:20s} : {row['importance']:.6f}")
        
        print(f"\nğŸ”» æœ€é‡è¦çš„è´Ÿé¢ç‰¹å¾ï¼ˆå¯¹è´Ÿé¢åˆ†ç±»è´¡çŒ®æœ€å¤§ï¼Œå‰{top_n}ä¸ªï¼‰:")
        top_negative = feature_importance.tail(top_n).iloc[::-1]
        for i, (idx, row) in enumerate(top_negative.iterrows(), 1):
            print(f"  {i:2d}. {row['feature']:20s} : {row['importance']:.6f}")
        
        # å¯è§†åŒ–å‰10ä¸ªæ­£é¢å’Œè´Ÿé¢ç‰¹å¾
        fig, axes = plt.subplots(1, 2, figsize=(14, 6))
        
        # æ­£é¢ç‰¹å¾
        top_pos = top_positive.head(10)
        axes[0].barh(range(len(top_pos)), top_pos['importance'].values)
        axes[0].set_yticks(range(len(top_pos)))
        axes[0].set_yticklabels(top_pos['feature'].values, fontsize=10)
        axes[0].invert_yaxis()
        axes[0].set_title('æœ€é‡è¦çš„æ­£é¢ç‰¹å¾', fontsize=14)
        axes[0].set_xlabel('ç³»æ•°å€¼', fontsize=12)
        
        # è´Ÿé¢ç‰¹å¾
        top_neg = top_negative.head(10)
        axes[1].barh(range(len(top_neg)), top_neg['importance'].values)
        axes[1].set_yticks(range(len(top_neg)))
        axes[1].set_yticklabels(top_neg['feature'].values, fontsize=10)
        axes[1].invert_yaxis()
        axes[1].set_title('æœ€é‡è¦çš„è´Ÿé¢ç‰¹å¾', fontsize=14)
        axes[1].set_xlabel('ç³»æ•°å€¼', fontsize=12)
        
        plt.suptitle('ç‰¹å¾é‡è¦æ€§åˆ†æ', fontsize=16, y=1.02)
        plt.tight_layout()
        plt.show()
    else:
        print("âš ï¸  å½“å‰æ¨¡å‹æ²¡æœ‰ç³»æ•°å±æ€§ï¼ˆå¯èƒ½ä½¿ç”¨äº†éçº¿æ€§æ ¸ï¼‰")

# è¿è¡Œç‰¹å¾é‡è¦æ€§åˆ†æ
analyze_feature_importance(svm_model, vectorizer, top_n=15)

# ==================== 9. æ–°è¯„è®ºé¢„æµ‹ ====================
def predict_new_reviews(model, vectorizer):
    """é¢„æµ‹æ–°è¯„è®ºçš„æƒ…æ„Ÿ"""
    print("\n" + "=" * 60)
    print("æ–°è¯„è®ºé¢„æµ‹")
    print("=" * 60)
    
    # ç¤ºä¾‹æ–°è¯„è®º
    new_reviews = [
        "è¿™ä¸ªäº§å“çœŸçš„å¤ªæ£’äº†ï¼è´¨é‡éå¸¸å¥½ï¼Œä½¿ç”¨èµ·æ¥éå¸¸æ–¹ä¾¿ï¼Œå¼ºçƒˆæ¨èï¼",
        "éå¸¸ç³Ÿç³•çš„è´­ç‰©ä½“éªŒï¼Œäº§å“è´¨é‡å·®ï¼Œå®¢æœæ€åº¦ä¹Ÿä¸å¥½ï¼Œå†ä¹Ÿä¸ä¼šä¹°äº†ã€‚",
        "è¿˜è¡Œå§ï¼Œä»·æ ¼ä¾¿å®œï¼Œä½†è´¨é‡ä¸€èˆ¬ï¼Œå¯¹å¾—èµ·è¿™ä¸ªä»·æ ¼ã€‚",
        "è¿™æ˜¯æˆ‘ä¹°è¿‡çš„æœ€å¥½çš„å•†å“ä¹‹ä¸€ï¼Œå®Œå…¨è¶…å‡ºäº†æˆ‘çš„æœŸæœ›ï¼Œæ€§ä»·æ¯”è¶…é«˜ï¼",
        "å•†å“æœ‰ç‘•ç–µï¼ŒåŒ…è£…ä¹Ÿç ´æŸäº†ï¼Œéå¸¸å¤±æœ›ï¼Œä¸æ¨èè´­ä¹°ã€‚",
        "ç‰©æµé€Ÿåº¦å¿«ï¼Œå•†å“åŒ…è£…å®Œå¥½ï¼Œä½¿ç”¨æ•ˆæœå¾ˆå¥½ï¼Œæ»¡æ„ï¼",
        "ä¸æè¿°ä¸ç¬¦ï¼Œå®ç‰©è´¨é‡å¾ˆå·®ï¼Œæ„Ÿè§‰è¢«éª—äº†ã€‚",
        "å®¢æœæœåŠ¡å¾ˆå¥½ï¼ŒåŠæ—¶è§£å†³é—®é¢˜ï¼Œå•†å“ä¹Ÿä¸é”™ã€‚",
        "ä»·æ ¼æœ‰ç‚¹è´µï¼Œä½†è´¨é‡ç¡®å®å¥½ï¼Œç‰©æœ‰æ‰€å€¼ã€‚",
        "æ ¹æœ¬ä¸èƒ½ç”¨ï¼Œå®Œå…¨æ˜¯åºŸå“ï¼Œè¦æ±‚é€€æ¬¾ï¼"
    ]
    
    # é¢„æµ‹å‡½æ•°
    def predict_single_review(review):
        """é¢„æµ‹å•ä¸ªè¯„è®º"""
        # é¢„å¤„ç†
        cleaned_review = chinese_preprocess(review)
        
        # è½¬æ¢ä¸ºç‰¹å¾å‘é‡
        review_vector = vectorizer.transform([cleaned_review])
        
        # é¢„æµ‹
        prediction = model.predict(review_vector)[0]
        proba = model.predict_proba(review_vector)[0]
        
        return prediction, proba
    
    print("é¢„æµ‹ç¤ºä¾‹è¯„è®ºçš„æƒ…æ„Ÿ:\n")
    
    for i, review in enumerate(new_reviews, 1):
        pred, proba = predict_single_review(review)
        sentiment = "æ­£é¢" if pred == 1 else "è´Ÿé¢"
        prob_positive = proba[1]
        
        # æ ¹æ®ç½®ä¿¡åº¦æ˜¾ç¤ºä¸åŒé¢œè‰²
        if (sentiment == "æ­£é¢" and prob_positive > 0.7) or (sentiment == "è´Ÿé¢" and prob_positive < 0.3):
            color = "\033[92m"  # é«˜ç½®ä¿¡åº¦ç”¨ç»¿è‰²
        elif (sentiment == "æ­£é¢" and prob_positive > 0.6) or (sentiment == "è´Ÿé¢" and prob_positive < 0.4):
            color = "\033[93m"  # ä¸­ç­‰ç½®ä¿¡åº¦ç”¨é»„è‰²
        else:
            color = "\033[91m"  # ä½ç½®ä¿¡åº¦ç”¨çº¢è‰²
        
        print(f"è¯„è®º {i}: {review[:50]}..." if len(review) > 50 else f"è¯„è®º {i}: {review}")
        print(f"{color}  é¢„æµ‹æƒ…æ„Ÿ: {sentiment}")
        print(f"  æ­£é¢æ¦‚ç‡: {prob_positive:.4f} | è´Ÿé¢æ¦‚ç‡: {proba[0]:.4f}\033[0m")
        print("-" * 60)

# è¿è¡Œæ–°è¯„è®ºé¢„æµ‹
predict_new_reviews(svm_model, vectorizer)

# ==================== 10. é¡¹ç›®æ€»ç»“ ====================
print("\n" + "=" * 60)
print("é¡¹ç›®æ€»ç»“")
print("=" * 60)
print("âœ“ æ•°æ®åŠ è½½: æˆåŠŸè¯»å–4ä¸ªæ•°æ®æ–‡ä»¶ï¼ˆè‡ªåŠ¨æ£€æµ‹å¹¶è§£æXMLæ ¼å¼ï¼‰")
print("âœ“ æ–‡æœ¬é¢„å¤„ç†: å®Œæˆä¸­æ–‡åˆ†è¯å’Œæ¸…æ´—")
print("âœ“ ç‰¹å¾æå–: ä½¿ç”¨TF-IDFæå–äº†æ–‡æœ¬ç‰¹å¾")
print("âœ“ æ¨¡å‹è®­ç»ƒ: æˆåŠŸè®­ç»ƒäº†SVMåˆ†ç±»å™¨")
print("âœ“ æ¨¡å‹è¯„ä¼°: åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°äº†æ¨¡å‹æ€§èƒ½")
print(f"\nğŸ‰ é¡¹ç›®å®Œæˆï¼æœ€ç»ˆæ¨¡å‹å‡†ç¡®ç‡: {accuracy:.4f}")
print("=" * 60)

# ==================== 11. æ¨¡å‹ä¿å­˜ï¼ˆå¯é€‰ï¼‰ ====================
# å¦‚æœéœ€è¦ä¿å­˜æ¨¡å‹ï¼Œå¯ä»¥å–æ¶ˆä»¥ä¸‹ä»£ç çš„æ³¨é‡Š
'''
import joblib
joblib.dump(svm_model, 'svm_sentiment_model.pkl')
joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')
print("\næ¨¡å‹å·²ä¿å­˜ä¸º svm_sentiment_model.pkl")
print("å‘é‡åŒ–å™¨å·²ä¿å­˜ä¸º tfidf_vectorizer.pkl")
'''


# In[ ]:





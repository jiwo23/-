#!/usr/bin/env python
# coding: utf-8

# In[31]:


# éªŒè¯åŸºç¡€ä¾èµ–ï¼ˆAnacondaé»˜è®¤å·²å®‰è£…ï¼Œæ— éœ€é¢å¤–å®‰è£…ï¼‰
import numpy as np
import networkx as nx
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

print("âœ… æ‰€æœ‰åŸºç¡€ä¾èµ–éªŒè¯é€šè¿‡ï¼")
print(f"numpyç‰ˆæœ¬ï¼š{np.__version__}")
print(f"networkxç‰ˆæœ¬ï¼š{nx.__version__}")


# In[64]:


import os
import re
import numpy as np
import networkx as nx
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

# ---------------------- 1. æ•°æ®åŠ è½½ï¼šè®­ç»ƒé›†/é¢„æµ‹é›†åˆ†ç¦» ----------------------
def load_train_data(train_file_paths):
    """
    åŠ è½½è®­ç»ƒé›†ï¼ˆsample.positive/negative.txtï¼‰ï¼šæ ‡ç­¾å›ºå®šï¼Œä»…ç”¨äºè®­ç»ƒ
    :param train_file_paths: å­—å…¸ {æ–‡ä»¶è·¯å¾„: å›ºå®šæ ‡ç­¾}
    :return: è®­ç»ƒæ•°æ® list[(review_id, text, label)], è®­ç»ƒé›†ç»Ÿè®¡
    """
    train_data = []
    train_stats = {}
    auto_id = 10000  # å”¯ä¸€IDèµ·å§‹å€¼

    for file_path, fixed_label in train_file_paths.items():
        if not os.path.exists(file_path):
            print(f"âš ï¸ è®­ç»ƒæ–‡ä»¶ {os.path.basename(file_path)} ä¸å­˜åœ¨ï¼Œè·³è¿‡")
            train_stats[file_path] = {"total": 0, "valid": 0}
            continue

        # è¯»å–æ–‡ä»¶
        try:
            with open(file_path, "rb") as f:
                content = f.read().decode("utf-8", errors="ignore")
            print(f"\nâœ… è¯»å–è®­ç»ƒæ–‡ä»¶ï¼š{os.path.basename(file_path)}")
        except Exception as e:
            print(f"âŒ è¯»å– {os.path.basename(file_path)} å¤±è´¥ï¼š{str(e)}")
            train_stats[file_path] = {"total": 0, "valid": 0}
            continue

        # æå–è¯„è®ºå†…å®¹ï¼ˆä»…åŒ¹é…<review>æ ‡ç­¾ï¼‰
        content_pattern = r'<review[^>]*>(.*?)</review>'
        content_matches = re.findall(content_pattern, content, re.DOTALL)
        valid_count = 0

        # å¤„ç†æ¯æ¡è¯„è®ºï¼ˆæ ‡ç­¾å›ºå®šä¸ºæ–‡ä»¶å¯¹åº”çš„æ­£/è´Ÿé¢ï¼‰
        for idx, text in enumerate(content_matches):
            # æ¸…ç†æ–‡æœ¬
            clean_text = re.sub(r'<[^>]+>', '', text.strip())
            clean_text = re.sub(r'\s+', ' ', clean_text).strip()
            if len(clean_text) < 5:
                continue
            # ç”Ÿæˆå”¯ä¸€ID
            review_id = auto_id + idx
            # æ ‡ç­¾å›ºå®šï¼ˆpositive=1ï¼Œnegative=0ï¼‰
            train_data.append((review_id, clean_text, fixed_label))
            valid_count += 1

        # è®°å½•ç»Ÿè®¡
        train_stats[file_path] = {
            "total": len(content_matches),
            "valid": valid_count
        }
        label_desc = "æ­£é¢(1)" if fixed_label == 1 else "è´Ÿé¢(0)"
        print(f"ğŸ”§ {os.path.basename(file_path)} æå–ï¼šæœ‰æ•ˆè¯„è®º {valid_count} æ¡ | æ ‡ç­¾å›ºå®šä¸º {label_desc}")

    # è®­ç»ƒé›†æ•´ä½“ç»Ÿè®¡
    total_train = len(train_data)
    total_pos = sum([1 for _, _, lab in train_data if lab == 1])
    total_neg = total_train - total_pos
    print(f"\nğŸ“Š è®­ç»ƒé›†æœ€ç»ˆç»Ÿè®¡ï¼š")
    print(f"æ€»æœ‰æ•ˆè¯„è®ºæ•°ï¼š{total_train} æ¡ | æ­£é¢(1) {total_pos} æ¡ | è´Ÿé¢(0) {total_neg} æ¡")

    # æ£€æŸ¥è®­ç»ƒé›†æœ‰æ•ˆæ€§ï¼ˆå¿…é¡»åŒ…å«æ­£è´Ÿä¸¤ç±»ï¼‰
    if total_pos == 0 or total_neg == 0:
        raise ValueError("âŒ è®­ç»ƒé›†å¿…é¡»åŒæ—¶åŒ…å«æ­£é¢å’Œè´Ÿé¢è¯„è®ºï¼è¯·æ£€æŸ¥sample.positive/negative.txtæ–‡ä»¶")

    return train_data, train_stats

def load_predict_data(predict_file_path):
    """
    åŠ è½½é¢„æµ‹é›†ï¼ˆtest.en.txtï¼‰ï¼šæ— æ ‡ç­¾ï¼Œä»…ç”¨äºé¢„æµ‹
    :param predict_file_path: é¢„æµ‹æ–‡ä»¶è·¯å¾„
    :return: é¢„æµ‹æ•°æ® list[(review_id, text)], é¢„æµ‹é›†ç»Ÿè®¡
    """
    predict_data = []
    auto_id = 20000  # é¢„æµ‹é›†IDèµ·å§‹å€¼ï¼ˆä¸è®­ç»ƒé›†åŒºåˆ†ï¼‰

    if not os.path.exists(predict_file_path):
        raise FileNotFoundError(f"âŒ é¢„æµ‹æ–‡ä»¶ {predict_file_path} ä¸å­˜åœ¨ï¼")

    # è¯»å–æ–‡ä»¶
    try:
        with open(predict_file_path, "rb") as f:
            content = f.read().decode("utf-8", errors="ignore")
        print(f"\nâœ… è¯»å–é¢„æµ‹æ–‡ä»¶ï¼š{os.path.basename(predict_file_path)}")
    except Exception as e:
        raise ValueError(f"âŒ è¯»å–é¢„æµ‹æ–‡ä»¶å¤±è´¥ï¼š{str(e)}")

    # æå–è¯„è®ºå†…å®¹
    content_pattern = r'<review[^>]*>(.*?)</review>'
    content_matches = re.findall(content_pattern, content, re.DOTALL)
    valid_count = 0

    # å¤„ç†æ¯æ¡è¯„è®ºï¼ˆæ— æ ‡ç­¾ï¼‰
    for idx, text in enumerate(content_matches):
        clean_text = re.sub(r'<[^>]+>', '', text.strip())
        clean_text = re.sub(r'\s+', ' ', clean_text).strip()
        if len(clean_text) < 5:
            continue
        review_id = auto_id + idx
        predict_data.append((review_id, clean_text))  # æ— æ ‡ç­¾
        valid_count += 1

    # ç»Ÿè®¡
    predict_stats = {
        "total": len(content_matches),
        "valid": valid_count
    }
    print(f"ğŸ”§ {os.path.basename(predict_file_path)} æå–ï¼šæœ‰æ•ˆè¯„è®º {valid_count} æ¡ | æ— æ ‡ç­¾ï¼ˆå¾…é¢„æµ‹ï¼‰")

    if valid_count == 0:
        raise ValueError("âŒ é¢„æµ‹é›†æ— æœ‰æ•ˆè¯„è®ºï¼è¯·æ£€æŸ¥test.en.txtæ–‡ä»¶æ ¼å¼")

    return predict_data, predict_stats

# ---------------------- 2. GNNç‰¹å¾æ„å»ºï¼ˆé€‚é…è®­ç»ƒ/é¢„æµ‹ï¼‰ ----------------------
def graph_convolution_aggregation(graph, node_features, alpha=0.8):
    """å›¾å·ç§¯èšåˆï¼šæ ¸å¿ƒé€»è¾‘"""
    if len(graph.nodes) == 0 or node_features.shape[0] == 0:
        return np.array([])
    
    num_nodes = len(graph.nodes)
    new_features = np.zeros_like(node_features)
    graph_nodes = sorted(list(graph.nodes))
    
    for idx, node_id in enumerate(graph_nodes):
        self_feat = node_features[idx]
        neighbors = list(graph.neighbors(node_id))
        neighbor_indices = [graph_nodes.index(n) for n in neighbors if n in graph_nodes]
        
        if neighbor_indices:
            neighbor_feat = node_features[neighbor_indices].mean(axis=0)
        else:
            neighbor_feat = np.zeros_like(self_feat)
        
        new_features[idx] = alpha * self_feat + (1 - alpha) * neighbor_feat
    return new_features

def build_gnn_features(texts, tfidf_model=None, fit_tfidf=True, vocab_size=5000):
    """
    æ„å»ºGNNç‰¹å¾
    :param texts: æ–‡æœ¬åˆ—è¡¨
    :param tfidf_model: è®­ç»ƒå¥½çš„TF-IDFæ¨¡å‹ï¼ˆé¢„æµ‹æ—¶ä¼ å…¥ï¼‰
    :param fit_tfidf: æ˜¯å¦è®­ç»ƒTF-IDFï¼ˆè®­ç»ƒé›†=Trueï¼Œé¢„æµ‹é›†=Falseï¼‰
    :return: GNNç‰¹å¾çŸ©é˜µ, TF-IDFæ¨¡å‹ï¼ˆä»…fit_tfidf=Trueæ—¶è¿”å›ï¼‰
    """
    if not texts:
        raise ValueError("âŒ æ— æ–‡æœ¬æ•°æ®ï¼Œæ— æ³•æ„å»ºGNNç‰¹å¾")

    # TF-IDFç‰¹å¾
    if fit_tfidf:
        tfidf = TfidfVectorizer(max_features=vocab_size, stop_words='english', max_df=0.95)
        tfidf_feat = tfidf.fit_transform(texts).toarray()
    else:
        if tfidf_model is None:
            raise ValueError("âŒ é¢„æµ‹æ—¶å¿…é¡»ä¼ å…¥è®­ç»ƒå¥½çš„TF-IDFæ¨¡å‹")
        tfidf_feat = tfidf_model.transform(texts).toarray()

    # æ„å»ºæ–‡æ¡£ç›¸ä¼¼åº¦å›¾ï¼ˆåˆ†æ‰¹è®¡ç®—ï¼Œé¿å…å†…å­˜æº¢å‡ºï¼‰
    graph = nx.Graph()
    num_docs = len(texts)
    graph.add_nodes_from(range(num_docs))
    
    if num_docs > 0:
        from sklearn.metrics.pairwise import cosine_similarity
        batch_size = 100
        for i in range(0, num_docs, batch_size):
            end_idx = min(i + batch_size, num_docs)
            sim_batch = cosine_similarity(tfidf_feat[i:end_idx], tfidf_feat)
            # ä»…ä¿ç•™é«˜ç›¸ä¼¼åº¦è¾¹
            for j in range(end_idx - i):
                for k in range(j + 1, num_docs):
                    if sim_batch[j][k] > 0.3:
                        graph.add_edge(i + j, k, weight=sim_batch[j][k])

    # å›¾å·ç§¯èšåˆ
    gnn_feat = graph_convolution_aggregation(graph, tfidf_feat)

    if fit_tfidf:
        return gnn_feat, tfidf
    else:
        return gnn_feat

# ---------------------- 3. æ¨¡å‹è®­ç»ƒ+è¯„ä¼°æŒ‡æ ‡ï¼ˆæ ¸å¿ƒæ–°å¢ï¼‰ ----------------------
def train_and_evaluate_model(train_gnn_feat, train_labels, val_size=0.2):
    """
    è®­ç»ƒæ¨¡å‹å¹¶è¾“å‡ºè¯„ä¼°æŒ‡æ ‡ï¼ˆå‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1ï¼‰
    :param train_gnn_feat: è®­ç»ƒé›†GNNç‰¹å¾
    :param train_labels: è®­ç»ƒé›†æ ‡ç­¾
    :param val_size: éªŒè¯é›†æ¯”ä¾‹
    :return: è®­ç»ƒå¥½çš„æ¨¡å‹ã€TF-IDFæ¨¡å‹
    """
    # æ‹†åˆ†è®­ç»ƒé›†/éªŒè¯é›†ï¼ˆç”¨äºè¯„ä¼°ï¼‰
    train_idx, val_idx = train_test_split(
        np.arange(len(train_gnn_feat)), 
        test_size=val_size, 
        random_state=42, 
        stratify=train_labels
    )
    train_feat_split = train_gnn_feat[train_idx]
    train_labels_split = train_labels[train_idx]
    val_feat_split = train_gnn_feat[val_idx]
    val_labels_split = train_labels[val_idx]

    # è®­ç»ƒæ¨¡å‹
    print("\nğŸš€ å¼€å§‹è®­ç»ƒGNNæ¨¡å‹ï¼ˆä»…ä½¿ç”¨sample.positive/negative.txtï¼‰...")
    model = LogisticRegression(max_iter=5000, class_weight='balanced', C=0.5)
    model.fit(train_feat_split, train_labels_split)
    print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼")

    # éªŒè¯é›†é¢„æµ‹
    val_pred = model.predict(val_feat_split)

    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    accuracy = accuracy_score(val_labels_split, val_pred)
    precision = precision_score(val_labels_split, val_pred, zero_division=0)
    recall = recall_score(val_labels_split, val_pred, zero_division=0)
    f1 = f1_score(val_labels_split, val_pred, zero_division=0)
    conf_mat = confusion_matrix(val_labels_split, val_pred)

    # è¾“å‡ºè¯„ä¼°æŠ¥å‘Š
    print("\n" + "="*80)
    print("ğŸ“Š æ¨¡å‹è®­ç»ƒè¯„ä¼°æŠ¥å‘Šï¼ˆéªŒè¯é›†ï¼‰")
    print("="*80)
    print(f"éªŒè¯é›†æ€»é‡ï¼š{len(val_labels_split)} æ¡")
    print(f"å‡†ç¡®ç‡ï¼ˆAccuracyï¼‰ï¼š{accuracy:.4f}")
    print(f"ç²¾ç¡®ç‡ï¼ˆPrecisionï¼‰ï¼š{precision:.4f}")
    print(f"å¬å›ç‡ï¼ˆRecallï¼‰ï¼š{recall:.4f}")
    print(f"F1åˆ†æ•°ï¼ˆF1-Scoreï¼‰ï¼š{f1:.4f}")

    # è¾“å‡ºæ··æ·†çŸ©é˜µ
    print("\næ··æ·†çŸ©é˜µï¼ˆè¡Œ=çœŸå®æ ‡ç­¾ï¼Œåˆ—=é¢„æµ‹æ ‡ç­¾ï¼‰ï¼š")
    print("                é¢„æµ‹è´Ÿé¢(0)      é¢„æµ‹æ­£é¢(1)")
    print(f"çœŸå®è´Ÿé¢(0)      {conf_mat[0][0]:^12d}      {conf_mat[0][1]:^12d}")
    print(f"çœŸå®æ­£é¢(1)      {conf_mat[1][0]:^12d}      {conf_mat[1][1]:^12d}")

    return model

# ---------------------- 4. å…¨æµç¨‹æ‰§è¡Œ ----------------------
if __name__ == "__main__":
    # é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆæ ¹æ®å®é™…è·¯å¾„ä¿®æ”¹ï¼‰
    TRAIN_FILES = {
        "sample.positive.txt": 1,  # å›ºå®šæ­£é¢æ ‡ç­¾
        "sample.negative.txt": 0   # å›ºå®šè´Ÿé¢æ ‡ç­¾
    }
    PREDICT_FILE = "test.en.txt"

    try:
        # 1. åŠ è½½è®­ç»ƒé›†ï¼ˆä»…sample.positive/negative.txtï¼‰
        train_data, train_stats = load_train_data(TRAIN_FILES)
        
        # 2. åŠ è½½é¢„æµ‹é›†ï¼ˆä»…test.en.txtï¼Œæ— æ ‡ç­¾ï¼‰
        predict_data, predict_stats = load_predict_data(PREDICT_FILE)

        # 3. æå–è®­ç»ƒé›†ç‰¹å¾å’Œæ ‡ç­¾
        train_texts = [x[1] for x in train_data]
        train_labels = np.array([x[2] for x in train_data])

        # 4. æ„å»ºè®­ç»ƒé›†GNNç‰¹å¾ï¼ˆè®­ç»ƒTF-IDFï¼‰
        train_gnn_feat, tfidf_model = build_gnn_features(
            train_texts, fit_tfidf=True, vocab_size=5000
        )

        # 5. è®­ç»ƒæ¨¡å‹å¹¶è¾“å‡ºè¯„ä¼°æŒ‡æ ‡ï¼ˆæ ¸å¿ƒæ–°å¢ï¼‰
        model = train_and_evaluate_model(train_gnn_feat, train_labels, val_size=0.2)

        # 6. æ„å»ºé¢„æµ‹é›†GNNç‰¹å¾ï¼ˆå¤ç”¨è®­ç»ƒå¥½çš„TF-IDFï¼‰
        predict_texts = [x[1] for x in predict_data]
        predict_gnn_feat = build_gnn_features(
            predict_texts, tfidf_model=tfidf_model, fit_tfidf=False
        )

        print("\nğŸ‰ å…¨æµç¨‹å®Œæˆï¼")
        print("- è®­ç»ƒé›†è¯„ä¼°æŒ‡æ ‡å·²è¾“å‡ºï¼ˆå‡†ç¡®ç‡/ç²¾ç¡®ç‡/å¬å›ç‡/F1ï¼‰")

    except Exception as e:
        print(f"\nâŒ æ‰§è¡Œå¤±è´¥ï¼š{str(e)}")


# In[ ]:




